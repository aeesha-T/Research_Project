{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on MDVR-KCL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanF0Hz</th>\n",
       "      <th>stdevF0Hz</th>\n",
       "      <th>HNR</th>\n",
       "      <th>localJitter</th>\n",
       "      <th>localabsoluteJitter</th>\n",
       "      <th>rapJitter</th>\n",
       "      <th>ppq5Jitter</th>\n",
       "      <th>localShimmer</th>\n",
       "      <th>localdbShimmer</th>\n",
       "      <th>apq3Shimmer</th>\n",
       "      <th>apq5Shimmer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>180.433976</td>\n",
       "      <td>51.653057</td>\n",
       "      <td>13.292053</td>\n",
       "      <td>0.027039</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.012019</td>\n",
       "      <td>0.013197</td>\n",
       "      <td>0.113439</td>\n",
       "      <td>1.124025</td>\n",
       "      <td>0.046161</td>\n",
       "      <td>0.067371</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>190.751972</td>\n",
       "      <td>34.887596</td>\n",
       "      <td>11.243993</td>\n",
       "      <td>0.016118</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.006764</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.079053</td>\n",
       "      <td>0.740666</td>\n",
       "      <td>0.029396</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>124.477366</td>\n",
       "      <td>26.621493</td>\n",
       "      <td>13.423983</td>\n",
       "      <td>0.026740</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.010862</td>\n",
       "      <td>0.011512</td>\n",
       "      <td>0.102565</td>\n",
       "      <td>0.974095</td>\n",
       "      <td>0.039042</td>\n",
       "      <td>0.058131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>182.557207</td>\n",
       "      <td>39.933612</td>\n",
       "      <td>12.235210</td>\n",
       "      <td>0.020701</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.008979</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>0.080388</td>\n",
       "      <td>0.811653</td>\n",
       "      <td>0.025759</td>\n",
       "      <td>0.037847</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>195.969796</td>\n",
       "      <td>41.446983</td>\n",
       "      <td>14.669165</td>\n",
       "      <td>0.015063</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.005371</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.082016</td>\n",
       "      <td>0.827343</td>\n",
       "      <td>0.029087</td>\n",
       "      <td>0.043951</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>168.012461</td>\n",
       "      <td>35.493510</td>\n",
       "      <td>13.020618</td>\n",
       "      <td>0.021790</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.009273</td>\n",
       "      <td>0.009862</td>\n",
       "      <td>0.078274</td>\n",
       "      <td>0.786047</td>\n",
       "      <td>0.024992</td>\n",
       "      <td>0.039414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>203.130952</td>\n",
       "      <td>48.609299</td>\n",
       "      <td>12.333993</td>\n",
       "      <td>0.027164</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.012624</td>\n",
       "      <td>0.013001</td>\n",
       "      <td>0.102844</td>\n",
       "      <td>0.994058</td>\n",
       "      <td>0.039932</td>\n",
       "      <td>0.053414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>197.503773</td>\n",
       "      <td>36.631657</td>\n",
       "      <td>15.683277</td>\n",
       "      <td>0.017656</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.007171</td>\n",
       "      <td>0.007674</td>\n",
       "      <td>0.078181</td>\n",
       "      <td>0.757354</td>\n",
       "      <td>0.029970</td>\n",
       "      <td>0.039306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>199.081279</td>\n",
       "      <td>41.081947</td>\n",
       "      <td>11.784297</td>\n",
       "      <td>0.024917</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.011396</td>\n",
       "      <td>0.011915</td>\n",
       "      <td>0.096619</td>\n",
       "      <td>0.963974</td>\n",
       "      <td>0.033895</td>\n",
       "      <td>0.049371</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>157.113264</td>\n",
       "      <td>46.419171</td>\n",
       "      <td>13.388906</td>\n",
       "      <td>0.034113</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.017499</td>\n",
       "      <td>0.016585</td>\n",
       "      <td>0.119624</td>\n",
       "      <td>1.104118</td>\n",
       "      <td>0.053728</td>\n",
       "      <td>0.058976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>163.844699</td>\n",
       "      <td>54.076942</td>\n",
       "      <td>11.548012</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.007439</td>\n",
       "      <td>0.009032</td>\n",
       "      <td>0.105805</td>\n",
       "      <td>0.993686</td>\n",
       "      <td>0.038722</td>\n",
       "      <td>0.060542</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>154.205413</td>\n",
       "      <td>42.577809</td>\n",
       "      <td>11.369774</td>\n",
       "      <td>0.029419</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>0.012156</td>\n",
       "      <td>0.089834</td>\n",
       "      <td>0.912168</td>\n",
       "      <td>0.033115</td>\n",
       "      <td>0.046434</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>160.093999</td>\n",
       "      <td>31.936264</td>\n",
       "      <td>12.994633</td>\n",
       "      <td>0.018726</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>0.080560</td>\n",
       "      <td>0.774723</td>\n",
       "      <td>0.029102</td>\n",
       "      <td>0.040420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>173.417455</td>\n",
       "      <td>34.365179</td>\n",
       "      <td>14.193625</td>\n",
       "      <td>0.022659</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.009854</td>\n",
       "      <td>0.010656</td>\n",
       "      <td>0.090751</td>\n",
       "      <td>0.878991</td>\n",
       "      <td>0.031667</td>\n",
       "      <td>0.046121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>179.048747</td>\n",
       "      <td>35.403221</td>\n",
       "      <td>16.224981</td>\n",
       "      <td>0.013899</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.005279</td>\n",
       "      <td>0.005674</td>\n",
       "      <td>0.072685</td>\n",
       "      <td>0.730909</td>\n",
       "      <td>0.027094</td>\n",
       "      <td>0.038795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>197.330931</td>\n",
       "      <td>39.729669</td>\n",
       "      <td>15.950713</td>\n",
       "      <td>0.016124</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>0.085010</td>\n",
       "      <td>0.810418</td>\n",
       "      <td>0.029995</td>\n",
       "      <td>0.044046</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>183.050353</td>\n",
       "      <td>46.792915</td>\n",
       "      <td>13.081668</td>\n",
       "      <td>0.024480</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.010880</td>\n",
       "      <td>0.010480</td>\n",
       "      <td>0.079695</td>\n",
       "      <td>0.767308</td>\n",
       "      <td>0.030681</td>\n",
       "      <td>0.039645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>194.704118</td>\n",
       "      <td>54.785527</td>\n",
       "      <td>13.076819</td>\n",
       "      <td>0.021499</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.009724</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>0.086963</td>\n",
       "      <td>0.876711</td>\n",
       "      <td>0.032313</td>\n",
       "      <td>0.044567</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>214.216909</td>\n",
       "      <td>38.864715</td>\n",
       "      <td>15.373564</td>\n",
       "      <td>0.016988</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.060868</td>\n",
       "      <td>0.629243</td>\n",
       "      <td>0.023691</td>\n",
       "      <td>0.028934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>182.554190</td>\n",
       "      <td>44.447825</td>\n",
       "      <td>15.092618</td>\n",
       "      <td>0.019678</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>0.009401</td>\n",
       "      <td>0.092794</td>\n",
       "      <td>0.876801</td>\n",
       "      <td>0.040549</td>\n",
       "      <td>0.051243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>137.759284</td>\n",
       "      <td>24.790170</td>\n",
       "      <td>11.083001</td>\n",
       "      <td>0.028160</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>0.013622</td>\n",
       "      <td>0.120958</td>\n",
       "      <td>1.129300</td>\n",
       "      <td>0.046355</td>\n",
       "      <td>0.066205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>123.414244</td>\n",
       "      <td>22.752103</td>\n",
       "      <td>12.358663</td>\n",
       "      <td>0.021337</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>0.009556</td>\n",
       "      <td>0.105540</td>\n",
       "      <td>1.004045</td>\n",
       "      <td>0.040983</td>\n",
       "      <td>0.061719</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>116.991841</td>\n",
       "      <td>58.387577</td>\n",
       "      <td>10.195438</td>\n",
       "      <td>0.038871</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.016799</td>\n",
       "      <td>0.017930</td>\n",
       "      <td>0.127226</td>\n",
       "      <td>1.246093</td>\n",
       "      <td>0.048545</td>\n",
       "      <td>0.071562</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>122.961628</td>\n",
       "      <td>22.484201</td>\n",
       "      <td>12.783982</td>\n",
       "      <td>0.027902</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.012136</td>\n",
       "      <td>0.012880</td>\n",
       "      <td>0.096135</td>\n",
       "      <td>0.972089</td>\n",
       "      <td>0.035746</td>\n",
       "      <td>0.051311</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>139.689970</td>\n",
       "      <td>46.669739</td>\n",
       "      <td>12.332291</td>\n",
       "      <td>0.025015</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.010713</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.121463</td>\n",
       "      <td>1.164322</td>\n",
       "      <td>0.046843</td>\n",
       "      <td>0.076398</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>173.221371</td>\n",
       "      <td>81.862069</td>\n",
       "      <td>11.459649</td>\n",
       "      <td>0.031018</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.015509</td>\n",
       "      <td>0.014327</td>\n",
       "      <td>0.179428</td>\n",
       "      <td>1.531849</td>\n",
       "      <td>0.070886</td>\n",
       "      <td>0.103642</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>210.237178</td>\n",
       "      <td>39.254275</td>\n",
       "      <td>15.979671</td>\n",
       "      <td>0.017851</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.007177</td>\n",
       "      <td>0.008796</td>\n",
       "      <td>0.082826</td>\n",
       "      <td>0.851598</td>\n",
       "      <td>0.026117</td>\n",
       "      <td>0.041526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>186.919188</td>\n",
       "      <td>43.036125</td>\n",
       "      <td>13.900163</td>\n",
       "      <td>0.021485</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.009976</td>\n",
       "      <td>0.011644</td>\n",
       "      <td>0.095878</td>\n",
       "      <td>1.032631</td>\n",
       "      <td>0.037534</td>\n",
       "      <td>0.055410</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>140.327577</td>\n",
       "      <td>48.573694</td>\n",
       "      <td>15.970827</td>\n",
       "      <td>0.030689</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.014995</td>\n",
       "      <td>0.014764</td>\n",
       "      <td>0.111399</td>\n",
       "      <td>1.092409</td>\n",
       "      <td>0.052731</td>\n",
       "      <td>0.065043</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>131.225704</td>\n",
       "      <td>23.870219</td>\n",
       "      <td>14.041177</td>\n",
       "      <td>0.024799</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.008344</td>\n",
       "      <td>0.010311</td>\n",
       "      <td>0.104867</td>\n",
       "      <td>1.005431</td>\n",
       "      <td>0.036322</td>\n",
       "      <td>0.055991</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>127.417091</td>\n",
       "      <td>15.856715</td>\n",
       "      <td>14.583647</td>\n",
       "      <td>0.024052</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.009932</td>\n",
       "      <td>0.010440</td>\n",
       "      <td>0.101346</td>\n",
       "      <td>0.977365</td>\n",
       "      <td>0.038380</td>\n",
       "      <td>0.053780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>99.176942</td>\n",
       "      <td>25.396425</td>\n",
       "      <td>13.423922</td>\n",
       "      <td>0.029025</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.013478</td>\n",
       "      <td>0.075952</td>\n",
       "      <td>0.787862</td>\n",
       "      <td>0.024354</td>\n",
       "      <td>0.038861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>133.525246</td>\n",
       "      <td>56.374227</td>\n",
       "      <td>14.355616</td>\n",
       "      <td>0.031816</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.014556</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.096488</td>\n",
       "      <td>0.993070</td>\n",
       "      <td>0.038624</td>\n",
       "      <td>0.049436</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>213.746231</td>\n",
       "      <td>45.818006</td>\n",
       "      <td>14.262968</td>\n",
       "      <td>0.020147</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.008710</td>\n",
       "      <td>0.009108</td>\n",
       "      <td>0.071151</td>\n",
       "      <td>0.752922</td>\n",
       "      <td>0.023605</td>\n",
       "      <td>0.034078</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>166.779713</td>\n",
       "      <td>30.941409</td>\n",
       "      <td>11.148522</td>\n",
       "      <td>0.025417</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.013308</td>\n",
       "      <td>0.012758</td>\n",
       "      <td>0.107019</td>\n",
       "      <td>1.000508</td>\n",
       "      <td>0.049595</td>\n",
       "      <td>0.057682</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>185.734636</td>\n",
       "      <td>36.970632</td>\n",
       "      <td>16.854876</td>\n",
       "      <td>0.020856</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.009839</td>\n",
       "      <td>0.010539</td>\n",
       "      <td>0.098915</td>\n",
       "      <td>1.047622</td>\n",
       "      <td>0.039979</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>185.303514</td>\n",
       "      <td>52.677109</td>\n",
       "      <td>14.082311</td>\n",
       "      <td>0.021292</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.089151</td>\n",
       "      <td>0.877028</td>\n",
       "      <td>0.033184</td>\n",
       "      <td>0.042282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      meanF0Hz  stdevF0Hz        HNR  localJitter  localabsoluteJitter  \\\n",
       "0   180.433976  51.653057  13.292053     0.027039             0.000151   \n",
       "1   190.751972  34.887596  11.243993     0.016118             0.000085   \n",
       "2   124.477366  26.621493  13.423983     0.026740             0.000216   \n",
       "3   182.557207  39.933612  12.235210     0.020701             0.000114   \n",
       "4   195.969796  41.446983  14.669165     0.015063             0.000077   \n",
       "5   168.012461  35.493510  13.020618     0.021790             0.000130   \n",
       "6   203.130952  48.609299  12.333993     0.027164             0.000134   \n",
       "7   197.503773  36.631657  15.683277     0.017656             0.000090   \n",
       "8   199.081279  41.081947  11.784297     0.024917             0.000125   \n",
       "9   157.113264  46.419171  13.388906     0.034113             0.000217   \n",
       "10  163.844699  54.076942  11.548012     0.018828             0.000115   \n",
       "11  154.205413  42.577809  11.369774     0.029419             0.000192   \n",
       "12  160.093999  31.936264  12.994633     0.018726             0.000117   \n",
       "13  173.417455  34.365179  14.193625     0.022659             0.000131   \n",
       "14  179.048747  35.403221  16.224981     0.013899             0.000078   \n",
       "15  197.330931  39.729669  15.950713     0.016124             0.000082   \n",
       "16  183.050353  46.792915  13.081668     0.024480             0.000134   \n",
       "17  194.704118  54.785527  13.076819     0.021499             0.000111   \n",
       "18  214.216909  38.864715  15.373564     0.016988             0.000079   \n",
       "19  182.554190  44.447825  15.092618     0.019678             0.000108   \n",
       "20  137.759284  24.790170  11.083001     0.028160             0.000205   \n",
       "21  123.414244  22.752103  12.358663     0.021337             0.000173   \n",
       "22  116.991841  58.387577  10.195438     0.038871             0.000332   \n",
       "23  122.961628  22.484201  12.783982     0.027902             0.000227   \n",
       "24  139.689970  46.669739  12.332291     0.025015             0.000179   \n",
       "25  173.221371  81.862069  11.459649     0.031018             0.000185   \n",
       "26  210.237178  39.254275  15.979671     0.017851             0.000085   \n",
       "27  186.919188  43.036125  13.900163     0.021485             0.000115   \n",
       "28  140.327577  48.573694  15.970827     0.030689             0.000219   \n",
       "29  131.225704  23.870219  14.041177     0.024799             0.000189   \n",
       "30  127.417091  15.856715  14.583647     0.024052             0.000189   \n",
       "31   99.176942  25.396425  13.423922     0.029025             0.000295   \n",
       "32  133.525246  56.374227  14.355616     0.031816             0.000241   \n",
       "33  213.746231  45.818006  14.262968     0.020147             0.000094   \n",
       "34  166.779713  30.941409  11.148522     0.025417             0.000153   \n",
       "35  185.734636  36.970632  16.854876     0.020856             0.000112   \n",
       "36  185.303514  52.677109  14.082311     0.021292             0.000115   \n",
       "\n",
       "    rapJitter  ppq5Jitter  localShimmer  localdbShimmer  apq3Shimmer  \\\n",
       "0    0.012019    0.013197      0.113439        1.124025     0.046161   \n",
       "1    0.006764    0.007168      0.079053        0.740666     0.029396   \n",
       "2    0.010862    0.011512      0.102565        0.974095     0.039042   \n",
       "3    0.008979    0.008983      0.080388        0.811653     0.025759   \n",
       "4    0.005371    0.006017      0.082016        0.827343     0.029087   \n",
       "5    0.009273    0.009862      0.078274        0.786047     0.024992   \n",
       "6    0.012624    0.013001      0.102844        0.994058     0.039932   \n",
       "7    0.007171    0.007674      0.078181        0.757354     0.029970   \n",
       "8    0.011396    0.011915      0.096619        0.963974     0.033895   \n",
       "9    0.017499    0.016585      0.119624        1.104118     0.053728   \n",
       "10   0.007439    0.009032      0.105805        0.993686     0.038722   \n",
       "11   0.013191    0.012156      0.089834        0.912168     0.033115   \n",
       "12   0.007586    0.008082      0.080560        0.774723     0.029102   \n",
       "13   0.009854    0.010656      0.090751        0.878991     0.031667   \n",
       "14   0.005279    0.005674      0.072685        0.730909     0.027094   \n",
       "15   0.005896    0.006771      0.085010        0.810418     0.029995   \n",
       "16   0.010880    0.010480      0.079695        0.767308     0.030681   \n",
       "17   0.009724    0.009703      0.086963        0.876711     0.032313   \n",
       "18   0.008453    0.008099      0.060868        0.629243     0.023691   \n",
       "19   0.009226    0.009401      0.092794        0.876801     0.040549   \n",
       "20   0.011123    0.013622      0.120958        1.129300     0.046355   \n",
       "21   0.008034    0.009556      0.105540        1.004045     0.040983   \n",
       "22   0.016799    0.017930      0.127226        1.246093     0.048545   \n",
       "23   0.012136    0.012880      0.096135        0.972089     0.035746   \n",
       "24   0.010713    0.013500      0.121463        1.164322     0.046843   \n",
       "25   0.015509    0.014327      0.179428        1.531849     0.070886   \n",
       "26   0.007177    0.008796      0.082826        0.851598     0.026117   \n",
       "27   0.009976    0.011644      0.095878        1.032631     0.037534   \n",
       "28   0.014995    0.014764      0.111399        1.092409     0.052731   \n",
       "29   0.008344    0.010311      0.104867        1.005431     0.036322   \n",
       "30   0.009932    0.010440      0.101346        0.977365     0.038380   \n",
       "31   0.009110    0.013478      0.075952        0.787862     0.024354   \n",
       "32   0.014556    0.015936      0.096488        0.993070     0.038624   \n",
       "33   0.008710    0.009108      0.071151        0.752922     0.023605   \n",
       "34   0.013308    0.012758      0.107019        1.000508     0.049595   \n",
       "35   0.009839    0.010539      0.098915        1.047622     0.039979   \n",
       "36   0.010419    0.009892      0.089151        0.877028     0.033184   \n",
       "\n",
       "    apq5Shimmer  label  \n",
       "0      0.067371      0  \n",
       "1      0.038308      0  \n",
       "2      0.058131      0  \n",
       "3      0.037847      0  \n",
       "4      0.043951      0  \n",
       "5      0.039414      0  \n",
       "6      0.053414      0  \n",
       "7      0.039306      0  \n",
       "8      0.049371      0  \n",
       "9      0.058976      0  \n",
       "10     0.060542      0  \n",
       "11     0.046434      0  \n",
       "12     0.040420      0  \n",
       "13     0.046121      0  \n",
       "14     0.038795      0  \n",
       "15     0.044046      0  \n",
       "16     0.039645      0  \n",
       "17     0.044567      0  \n",
       "18     0.028934      0  \n",
       "19     0.051243      0  \n",
       "20     0.066205      0  \n",
       "21     0.061719      1  \n",
       "22     0.071562      1  \n",
       "23     0.051311      1  \n",
       "24     0.076398      1  \n",
       "25     0.103642      1  \n",
       "26     0.041526      1  \n",
       "27     0.055410      1  \n",
       "28     0.065043      1  \n",
       "29     0.055991      1  \n",
       "30     0.053780      1  \n",
       "31     0.038861      1  \n",
       "32     0.049436      1  \n",
       "33     0.034078      1  \n",
       "34     0.057682      1  \n",
       "35     0.052948      1  \n",
       "36     0.042282      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./readtext.csv\")\n",
    "#df = shuffle(df)\n",
    "#df.reset_index(inplace=True, drop=True)\n",
    "df.drop('voiceID', inplace = True, axis = 1)\n",
    "df['label'].value_counts()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>185.303514</td>\n",
       "      <td>52.677109</td>\n",
       "      <td>14.082311</td>\n",
       "      <td>0.021292</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.089151</td>\n",
       "      <td>0.877028</td>\n",
       "      <td>0.033184</td>\n",
       "      <td>0.042282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>166.779713</td>\n",
       "      <td>30.941409</td>\n",
       "      <td>11.148522</td>\n",
       "      <td>0.025417</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.013308</td>\n",
       "      <td>0.012758</td>\n",
       "      <td>0.107019</td>\n",
       "      <td>1.000508</td>\n",
       "      <td>0.049595</td>\n",
       "      <td>0.057682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>131.225704</td>\n",
       "      <td>23.870219</td>\n",
       "      <td>14.041177</td>\n",
       "      <td>0.024799</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.008344</td>\n",
       "      <td>0.010311</td>\n",
       "      <td>0.104867</td>\n",
       "      <td>1.005431</td>\n",
       "      <td>0.036322</td>\n",
       "      <td>0.055991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>213.746231</td>\n",
       "      <td>45.818006</td>\n",
       "      <td>14.262968</td>\n",
       "      <td>0.020147</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.008710</td>\n",
       "      <td>0.009108</td>\n",
       "      <td>0.071151</td>\n",
       "      <td>0.752922</td>\n",
       "      <td>0.023605</td>\n",
       "      <td>0.034078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>199.081279</td>\n",
       "      <td>41.081947</td>\n",
       "      <td>11.784297</td>\n",
       "      <td>0.024917</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.011396</td>\n",
       "      <td>0.011915</td>\n",
       "      <td>0.096619</td>\n",
       "      <td>0.963974</td>\n",
       "      <td>0.033895</td>\n",
       "      <td>0.049371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>173.417455</td>\n",
       "      <td>34.365179</td>\n",
       "      <td>14.193625</td>\n",
       "      <td>0.022659</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.009854</td>\n",
       "      <td>0.010656</td>\n",
       "      <td>0.090751</td>\n",
       "      <td>0.878991</td>\n",
       "      <td>0.031667</td>\n",
       "      <td>0.046121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>168.012461</td>\n",
       "      <td>35.493510</td>\n",
       "      <td>13.020618</td>\n",
       "      <td>0.021790</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.009273</td>\n",
       "      <td>0.009862</td>\n",
       "      <td>0.078274</td>\n",
       "      <td>0.786047</td>\n",
       "      <td>0.024992</td>\n",
       "      <td>0.039414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>194.704118</td>\n",
       "      <td>54.785527</td>\n",
       "      <td>13.076819</td>\n",
       "      <td>0.021499</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.009724</td>\n",
       "      <td>0.009703</td>\n",
       "      <td>0.086963</td>\n",
       "      <td>0.876711</td>\n",
       "      <td>0.032313</td>\n",
       "      <td>0.044567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>179.048747</td>\n",
       "      <td>35.403221</td>\n",
       "      <td>16.224981</td>\n",
       "      <td>0.013899</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.005279</td>\n",
       "      <td>0.005674</td>\n",
       "      <td>0.072685</td>\n",
       "      <td>0.730909</td>\n",
       "      <td>0.027094</td>\n",
       "      <td>0.038795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>197.503773</td>\n",
       "      <td>36.631657</td>\n",
       "      <td>15.683277</td>\n",
       "      <td>0.017656</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.007171</td>\n",
       "      <td>0.007674</td>\n",
       "      <td>0.078181</td>\n",
       "      <td>0.757354</td>\n",
       "      <td>0.029970</td>\n",
       "      <td>0.039306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>186.919188</td>\n",
       "      <td>43.036125</td>\n",
       "      <td>13.900163</td>\n",
       "      <td>0.021485</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.009976</td>\n",
       "      <td>0.011644</td>\n",
       "      <td>0.095878</td>\n",
       "      <td>1.032631</td>\n",
       "      <td>0.037534</td>\n",
       "      <td>0.055410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>190.751972</td>\n",
       "      <td>34.887596</td>\n",
       "      <td>11.243993</td>\n",
       "      <td>0.016118</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.006764</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.079053</td>\n",
       "      <td>0.740666</td>\n",
       "      <td>0.029396</td>\n",
       "      <td>0.038308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>160.093999</td>\n",
       "      <td>31.936264</td>\n",
       "      <td>12.994633</td>\n",
       "      <td>0.018726</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.007586</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>0.080560</td>\n",
       "      <td>0.774723</td>\n",
       "      <td>0.029102</td>\n",
       "      <td>0.040420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>210.237178</td>\n",
       "      <td>39.254275</td>\n",
       "      <td>15.979671</td>\n",
       "      <td>0.017851</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.007177</td>\n",
       "      <td>0.008796</td>\n",
       "      <td>0.082826</td>\n",
       "      <td>0.851598</td>\n",
       "      <td>0.026117</td>\n",
       "      <td>0.041526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>139.689970</td>\n",
       "      <td>46.669739</td>\n",
       "      <td>12.332291</td>\n",
       "      <td>0.025015</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.010713</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.121463</td>\n",
       "      <td>1.164322</td>\n",
       "      <td>0.046843</td>\n",
       "      <td>0.076398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>203.130952</td>\n",
       "      <td>48.609299</td>\n",
       "      <td>12.333993</td>\n",
       "      <td>0.027164</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.012624</td>\n",
       "      <td>0.013001</td>\n",
       "      <td>0.102844</td>\n",
       "      <td>0.994058</td>\n",
       "      <td>0.039932</td>\n",
       "      <td>0.053414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>122.961628</td>\n",
       "      <td>22.484201</td>\n",
       "      <td>12.783982</td>\n",
       "      <td>0.027902</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.012136</td>\n",
       "      <td>0.012880</td>\n",
       "      <td>0.096135</td>\n",
       "      <td>0.972089</td>\n",
       "      <td>0.035746</td>\n",
       "      <td>0.051311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>195.969796</td>\n",
       "      <td>41.446983</td>\n",
       "      <td>14.669165</td>\n",
       "      <td>0.015063</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.005371</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.082016</td>\n",
       "      <td>0.827343</td>\n",
       "      <td>0.029087</td>\n",
       "      <td>0.043951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>214.216909</td>\n",
       "      <td>38.864715</td>\n",
       "      <td>15.373564</td>\n",
       "      <td>0.016988</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.060868</td>\n",
       "      <td>0.629243</td>\n",
       "      <td>0.023691</td>\n",
       "      <td>0.028934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>123.414244</td>\n",
       "      <td>22.752103</td>\n",
       "      <td>12.358663</td>\n",
       "      <td>0.021337</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>0.009556</td>\n",
       "      <td>0.105540</td>\n",
       "      <td>1.004045</td>\n",
       "      <td>0.040983</td>\n",
       "      <td>0.061719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>182.554190</td>\n",
       "      <td>44.447825</td>\n",
       "      <td>15.092618</td>\n",
       "      <td>0.019678</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>0.009401</td>\n",
       "      <td>0.092794</td>\n",
       "      <td>0.876801</td>\n",
       "      <td>0.040549</td>\n",
       "      <td>0.051243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>157.113264</td>\n",
       "      <td>46.419171</td>\n",
       "      <td>13.388906</td>\n",
       "      <td>0.034113</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.017499</td>\n",
       "      <td>0.016585</td>\n",
       "      <td>0.119624</td>\n",
       "      <td>1.104118</td>\n",
       "      <td>0.053728</td>\n",
       "      <td>0.058976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>185.734636</td>\n",
       "      <td>36.970632</td>\n",
       "      <td>16.854876</td>\n",
       "      <td>0.020856</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.009839</td>\n",
       "      <td>0.010539</td>\n",
       "      <td>0.098915</td>\n",
       "      <td>1.047622</td>\n",
       "      <td>0.039979</td>\n",
       "      <td>0.052948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>182.557207</td>\n",
       "      <td>39.933612</td>\n",
       "      <td>12.235210</td>\n",
       "      <td>0.020701</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.008979</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>0.080388</td>\n",
       "      <td>0.811653</td>\n",
       "      <td>0.025759</td>\n",
       "      <td>0.037847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>180.433976</td>\n",
       "      <td>51.653057</td>\n",
       "      <td>13.292053</td>\n",
       "      <td>0.027039</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.012019</td>\n",
       "      <td>0.013197</td>\n",
       "      <td>0.113439</td>\n",
       "      <td>1.124025</td>\n",
       "      <td>0.046161</td>\n",
       "      <td>0.067371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2         3         4         5         6  \\\n",
       "0   185.303514  52.677109  14.082311  0.021292  0.000115  0.010419  0.009892   \n",
       "1   166.779713  30.941409  11.148522  0.025417  0.000153  0.013308  0.012758   \n",
       "2   131.225704  23.870219  14.041177  0.024799  0.000189  0.008344  0.010311   \n",
       "3   213.746231  45.818006  14.262968  0.020147  0.000094  0.008710  0.009108   \n",
       "4   199.081279  41.081947  11.784297  0.024917  0.000125  0.011396  0.011915   \n",
       "5   173.417455  34.365179  14.193625  0.022659  0.000131  0.009854  0.010656   \n",
       "6   168.012461  35.493510  13.020618  0.021790  0.000130  0.009273  0.009862   \n",
       "7   194.704118  54.785527  13.076819  0.021499  0.000111  0.009724  0.009703   \n",
       "8   179.048747  35.403221  16.224981  0.013899  0.000078  0.005279  0.005674   \n",
       "9   197.503773  36.631657  15.683277  0.017656  0.000090  0.007171  0.007674   \n",
       "10  186.919188  43.036125  13.900163  0.021485  0.000115  0.009976  0.011644   \n",
       "11  190.751972  34.887596  11.243993  0.016118  0.000085  0.006764  0.007168   \n",
       "12  160.093999  31.936264  12.994633  0.018726  0.000117  0.007586  0.008082   \n",
       "13  210.237178  39.254275  15.979671  0.017851  0.000085  0.007177  0.008796   \n",
       "14  139.689970  46.669739  12.332291  0.025015  0.000179  0.010713  0.013500   \n",
       "15  203.130952  48.609299  12.333993  0.027164  0.000134  0.012624  0.013001   \n",
       "16  122.961628  22.484201  12.783982  0.027902  0.000227  0.012136  0.012880   \n",
       "17  195.969796  41.446983  14.669165  0.015063  0.000077  0.005371  0.006017   \n",
       "18  214.216909  38.864715  15.373564  0.016988  0.000079  0.008453  0.008099   \n",
       "19  123.414244  22.752103  12.358663  0.021337  0.000173  0.008034  0.009556   \n",
       "20  182.554190  44.447825  15.092618  0.019678  0.000108  0.009226  0.009401   \n",
       "21  157.113264  46.419171  13.388906  0.034113  0.000217  0.017499  0.016585   \n",
       "22  185.734636  36.970632  16.854876  0.020856  0.000112  0.009839  0.010539   \n",
       "23  182.557207  39.933612  12.235210  0.020701  0.000114  0.008979  0.008983   \n",
       "24  180.433976  51.653057  13.292053  0.027039  0.000151  0.012019  0.013197   \n",
       "\n",
       "           7         8         9        10  \n",
       "0   0.089151  0.877028  0.033184  0.042282  \n",
       "1   0.107019  1.000508  0.049595  0.057682  \n",
       "2   0.104867  1.005431  0.036322  0.055991  \n",
       "3   0.071151  0.752922  0.023605  0.034078  \n",
       "4   0.096619  0.963974  0.033895  0.049371  \n",
       "5   0.090751  0.878991  0.031667  0.046121  \n",
       "6   0.078274  0.786047  0.024992  0.039414  \n",
       "7   0.086963  0.876711  0.032313  0.044567  \n",
       "8   0.072685  0.730909  0.027094  0.038795  \n",
       "9   0.078181  0.757354  0.029970  0.039306  \n",
       "10  0.095878  1.032631  0.037534  0.055410  \n",
       "11  0.079053  0.740666  0.029396  0.038308  \n",
       "12  0.080560  0.774723  0.029102  0.040420  \n",
       "13  0.082826  0.851598  0.026117  0.041526  \n",
       "14  0.121463  1.164322  0.046843  0.076398  \n",
       "15  0.102844  0.994058  0.039932  0.053414  \n",
       "16  0.096135  0.972089  0.035746  0.051311  \n",
       "17  0.082016  0.827343  0.029087  0.043951  \n",
       "18  0.060868  0.629243  0.023691  0.028934  \n",
       "19  0.105540  1.004045  0.040983  0.061719  \n",
       "20  0.092794  0.876801  0.040549  0.051243  \n",
       "21  0.119624  1.104118  0.053728  0.058976  \n",
       "22  0.098915  1.047622  0.039979  0.052948  \n",
       "23  0.080388  0.811653  0.025759  0.037847  \n",
       "24  0.113439  1.124025  0.046161  0.067371  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#separate dependent and independent variable\n",
    "X = df.iloc[:, :-1]\n",
    "df_X = df.iloc[:, :-1].values\n",
    "df_Y = df.iloc[:,-1].values\n",
    "\n",
    "# Split the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_Y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "pd.DataFrame(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.683159</td>\n",
       "      <td>0.934727</td>\n",
       "      <td>0.514127</td>\n",
       "      <td>0.365714</td>\n",
       "      <td>0.252388</td>\n",
       "      <td>0.420615</td>\n",
       "      <td>0.386582</td>\n",
       "      <td>0.466749</td>\n",
       "      <td>0.463082</td>\n",
       "      <td>0.318015</td>\n",
       "      <td>0.281224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.480170</td>\n",
       "      <td>0.261822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.569791</td>\n",
       "      <td>0.502888</td>\n",
       "      <td>0.657009</td>\n",
       "      <td>0.649233</td>\n",
       "      <td>0.761632</td>\n",
       "      <td>0.693850</td>\n",
       "      <td>0.862800</td>\n",
       "      <td>0.605668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.090560</td>\n",
       "      <td>0.042909</td>\n",
       "      <td>0.506918</td>\n",
       "      <td>0.539225</td>\n",
       "      <td>0.746808</td>\n",
       "      <td>0.250842</td>\n",
       "      <td>0.425005</td>\n",
       "      <td>0.726118</td>\n",
       "      <td>0.703052</td>\n",
       "      <td>0.422196</td>\n",
       "      <td>0.570045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.994842</td>\n",
       "      <td>0.722379</td>\n",
       "      <td>0.545786</td>\n",
       "      <td>0.309113</td>\n",
       "      <td>0.114155</td>\n",
       "      <td>0.280805</td>\n",
       "      <td>0.314770</td>\n",
       "      <td>0.169704</td>\n",
       "      <td>0.231142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.834140</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.111415</td>\n",
       "      <td>0.545052</td>\n",
       "      <td>0.320417</td>\n",
       "      <td>0.500617</td>\n",
       "      <td>0.571964</td>\n",
       "      <td>0.589996</td>\n",
       "      <td>0.625574</td>\n",
       "      <td>0.341612</td>\n",
       "      <td>0.430574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.552909</td>\n",
       "      <td>0.367817</td>\n",
       "      <td>0.533634</td>\n",
       "      <td>0.433380</td>\n",
       "      <td>0.359524</td>\n",
       "      <td>0.374398</td>\n",
       "      <td>0.456565</td>\n",
       "      <td>0.493161</td>\n",
       "      <td>0.466750</td>\n",
       "      <td>0.267652</td>\n",
       "      <td>0.362105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.493679</td>\n",
       "      <td>0.402748</td>\n",
       "      <td>0.328072</td>\n",
       "      <td>0.390382</td>\n",
       "      <td>0.350274</td>\n",
       "      <td>0.326849</td>\n",
       "      <td>0.383806</td>\n",
       "      <td>0.287253</td>\n",
       "      <td>0.293049</td>\n",
       "      <td>0.046075</td>\n",
       "      <td>0.220798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337921</td>\n",
       "      <td>0.375990</td>\n",
       "      <td>0.222888</td>\n",
       "      <td>0.363734</td>\n",
       "      <td>0.369304</td>\n",
       "      <td>0.430656</td>\n",
       "      <td>0.462488</td>\n",
       "      <td>0.289103</td>\n",
       "      <td>0.329359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.614618</td>\n",
       "      <td>0.399953</td>\n",
       "      <td>0.889615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195013</td>\n",
       "      <td>0.190002</td>\n",
       "      <td>0.115829</td>\n",
       "      <td>0.207765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.816853</td>\n",
       "      <td>0.437984</td>\n",
       "      <td>0.794685</td>\n",
       "      <td>0.185842</td>\n",
       "      <td>0.083057</td>\n",
       "      <td>0.154837</td>\n",
       "      <td>0.183334</td>\n",
       "      <td>0.285719</td>\n",
       "      <td>0.239425</td>\n",
       "      <td>0.211324</td>\n",
       "      <td>0.218521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.700864</td>\n",
       "      <td>0.636256</td>\n",
       "      <td>0.482207</td>\n",
       "      <td>0.375300</td>\n",
       "      <td>0.252123</td>\n",
       "      <td>0.384398</td>\n",
       "      <td>0.547164</td>\n",
       "      <td>0.577768</td>\n",
       "      <td>0.753885</td>\n",
       "      <td>0.462431</td>\n",
       "      <td>0.557817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.742865</td>\n",
       "      <td>0.383990</td>\n",
       "      <td>0.016731</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.049823</td>\n",
       "      <td>0.121526</td>\n",
       "      <td>0.136899</td>\n",
       "      <td>0.300104</td>\n",
       "      <td>0.208236</td>\n",
       "      <td>0.192255</td>\n",
       "      <td>0.197484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.406907</td>\n",
       "      <td>0.292622</td>\n",
       "      <td>0.323518</td>\n",
       "      <td>0.238807</td>\n",
       "      <td>0.267768</td>\n",
       "      <td>0.188812</td>\n",
       "      <td>0.220659</td>\n",
       "      <td>0.324978</td>\n",
       "      <td>0.271886</td>\n",
       "      <td>0.182494</td>\n",
       "      <td>0.241995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.956389</td>\n",
       "      <td>0.519176</td>\n",
       "      <td>0.846626</td>\n",
       "      <td>0.195491</td>\n",
       "      <td>0.053206</td>\n",
       "      <td>0.155377</td>\n",
       "      <td>0.286094</td>\n",
       "      <td>0.362372</td>\n",
       "      <td>0.415555</td>\n",
       "      <td>0.083407</td>\n",
       "      <td>0.265298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.183314</td>\n",
       "      <td>0.748748</td>\n",
       "      <td>0.207447</td>\n",
       "      <td>0.549912</td>\n",
       "      <td>0.678122</td>\n",
       "      <td>0.444735</td>\n",
       "      <td>0.717190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.771460</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.878517</td>\n",
       "      <td>0.808793</td>\n",
       "      <td>0.207746</td>\n",
       "      <td>0.656238</td>\n",
       "      <td>0.375828</td>\n",
       "      <td>0.601039</td>\n",
       "      <td>0.671499</td>\n",
       "      <td>0.692733</td>\n",
       "      <td>0.681796</td>\n",
       "      <td>0.542009</td>\n",
       "      <td>0.515748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.286603</td>\n",
       "      <td>0.692708</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.561147</td>\n",
       "      <td>0.660458</td>\n",
       "      <td>0.582011</td>\n",
       "      <td>0.640740</td>\n",
       "      <td>0.403060</td>\n",
       "      <td>0.471447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.800043</td>\n",
       "      <td>0.587059</td>\n",
       "      <td>0.616969</td>\n",
       "      <td>0.057579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>0.031460</td>\n",
       "      <td>0.349004</td>\n",
       "      <td>0.370225</td>\n",
       "      <td>0.181998</td>\n",
       "      <td>0.316385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.507116</td>\n",
       "      <td>0.740410</td>\n",
       "      <td>0.152838</td>\n",
       "      <td>0.014238</td>\n",
       "      <td>0.259766</td>\n",
       "      <td>0.222270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.008294</td>\n",
       "      <td>0.212069</td>\n",
       "      <td>0.367974</td>\n",
       "      <td>0.640116</td>\n",
       "      <td>0.225463</td>\n",
       "      <td>0.355771</td>\n",
       "      <td>0.737220</td>\n",
       "      <td>0.700462</td>\n",
       "      <td>0.576919</td>\n",
       "      <td>0.690740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.653031</td>\n",
       "      <td>0.679960</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.285881</td>\n",
       "      <td>0.203509</td>\n",
       "      <td>0.322988</td>\n",
       "      <td>0.341569</td>\n",
       "      <td>0.526876</td>\n",
       "      <td>0.462657</td>\n",
       "      <td>0.562512</td>\n",
       "      <td>0.470020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.374243</td>\n",
       "      <td>0.740990</td>\n",
       "      <td>0.392612</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.930363</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969657</td>\n",
       "      <td>0.887486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.632931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.687884</td>\n",
       "      <td>0.448478</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.344184</td>\n",
       "      <td>0.233918</td>\n",
       "      <td>0.373195</td>\n",
       "      <td>0.445853</td>\n",
       "      <td>0.627894</td>\n",
       "      <td>0.781902</td>\n",
       "      <td>0.543576</td>\n",
       "      <td>0.505942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.653064</td>\n",
       "      <td>0.540207</td>\n",
       "      <td>0.190435</td>\n",
       "      <td>0.336495</td>\n",
       "      <td>0.243318</td>\n",
       "      <td>0.302831</td>\n",
       "      <td>0.303238</td>\n",
       "      <td>0.322142</td>\n",
       "      <td>0.340903</td>\n",
       "      <td>0.071511</td>\n",
       "      <td>0.187788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.629798</td>\n",
       "      <td>0.903023</td>\n",
       "      <td>0.375639</td>\n",
       "      <td>0.650029</td>\n",
       "      <td>0.490687</td>\n",
       "      <td>0.551607</td>\n",
       "      <td>0.689425</td>\n",
       "      <td>0.867586</td>\n",
       "      <td>0.924690</td>\n",
       "      <td>0.748823</td>\n",
       "      <td>0.809812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.683159  0.934727  0.514127  0.365714  0.252388  0.420615  0.386582   \n",
       "1   0.480170  0.261822  0.000000  0.569791  0.502888  0.657009  0.649233   \n",
       "2   0.090560  0.042909  0.506918  0.539225  0.746808  0.250842  0.425005   \n",
       "3   0.994842  0.722379  0.545786  0.309113  0.114155  0.280805  0.314770   \n",
       "4   0.834140  0.575758  0.111415  0.545052  0.320417  0.500617  0.571964   \n",
       "5   0.552909  0.367817  0.533634  0.433380  0.359524  0.374398  0.456565   \n",
       "6   0.493679  0.402748  0.328072  0.390382  0.350274  0.326849  0.383806   \n",
       "7   0.786174  1.000000  0.337921  0.375990  0.222888  0.363734  0.369304   \n",
       "8   0.614618  0.399953  0.889615  0.000000  0.004697  0.000000  0.000000   \n",
       "9   0.816853  0.437984  0.794685  0.185842  0.083057  0.154837  0.183334   \n",
       "10  0.700864  0.636256  0.482207  0.375300  0.252123  0.384398  0.547164   \n",
       "11  0.742865  0.383990  0.016731  0.109800  0.049823  0.121526  0.136899   \n",
       "12  0.406907  0.292622  0.323518  0.238807  0.267768  0.188812  0.220659   \n",
       "13  0.956389  0.519176  0.846626  0.195491  0.053206  0.155377  0.286094   \n",
       "14  0.183314  0.748748  0.207447  0.549912  0.678122  0.444735  0.717190   \n",
       "15  0.878517  0.808793  0.207746  0.656238  0.375828  0.601039  0.671499   \n",
       "16  0.000000  0.000000  0.286603  0.692708  1.000000  0.561147  0.660458   \n",
       "17  0.800043  0.587059  0.616969  0.057579  0.000000  0.007527  0.031460   \n",
       "18  1.000000  0.507116  0.740410  0.152838  0.014238  0.259766  0.222270   \n",
       "19  0.004960  0.008294  0.212069  0.367974  0.640116  0.225463  0.355771   \n",
       "20  0.653031  0.679960  0.691176  0.285881  0.203509  0.322988  0.341569   \n",
       "21  0.374243  0.740990  0.392612  1.000000  0.930363  1.000000  1.000000   \n",
       "22  0.687884  0.448478  1.000000  0.344184  0.233918  0.373195  0.445853   \n",
       "23  0.653064  0.540207  0.190435  0.336495  0.243318  0.302831  0.303238   \n",
       "24  0.629798  0.903023  0.375639  0.650029  0.490687  0.551607  0.689425   \n",
       "\n",
       "           7         8         9        10  \n",
       "0   0.466749  0.463082  0.318015  0.281224  \n",
       "1   0.761632  0.693850  0.862800  0.605668  \n",
       "2   0.726118  0.703052  0.422196  0.570045  \n",
       "3   0.169704  0.231142  0.000000  0.108383  \n",
       "4   0.589996  0.625574  0.341612  0.430574  \n",
       "5   0.493161  0.466750  0.267652  0.362105  \n",
       "6   0.287253  0.293049  0.046075  0.220798  \n",
       "7   0.430656  0.462488  0.289103  0.329359  \n",
       "8   0.195013  0.190002  0.115829  0.207765  \n",
       "9   0.285719  0.239425  0.211324  0.218521  \n",
       "10  0.577768  0.753885  0.462431  0.557817  \n",
       "11  0.300104  0.208236  0.192255  0.197484  \n",
       "12  0.324978  0.271886  0.182494  0.241995  \n",
       "13  0.362372  0.415555  0.083407  0.265298  \n",
       "14  1.000000  1.000000  0.771460  1.000000  \n",
       "15  0.692733  0.681796  0.542009  0.515748  \n",
       "16  0.582011  0.640740  0.403060  0.471447  \n",
       "17  0.349004  0.370225  0.181998  0.316385  \n",
       "18  0.000000  0.000000  0.002874  0.000000  \n",
       "19  0.737220  0.700462  0.576919  0.690740  \n",
       "20  0.526876  0.462657  0.562512  0.470020  \n",
       "21  0.969657  0.887486  1.000000  0.632931  \n",
       "22  0.627894  0.781902  0.543576  0.505942  \n",
       "23  0.322142  0.340903  0.071511  0.187788  \n",
       "24  0.867586  0.924690  0.748823  0.809812  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale\n",
    "#sc = StandardScaler()\n",
    "sc = MinMaxScaler()\n",
    "#sc = RobustScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *****************KNN Experiments******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.66666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         6\n",
      "           1       0.67      0.67      0.67         6\n",
      "\n",
      "    accuracy                           0.67        12\n",
      "   macro avg       0.67      0.67      0.67        12\n",
      "weighted avg       0.67      0.67      0.67        12\n",
      "\n",
      "0.6666666666666667\n",
      "[1 1 0 0 0 1 0 0 1 0 1 1]\n",
      "[[4 2]\n",
      " [2 4]]\n"
     ]
    }
   ],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#KNN\n",
    "model_knn = KNeighborsClassifier(n_neighbors = 10)\n",
    "model_knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = model_knn.predict(X_test)\n",
    "\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "accuracy_knn = ((conf_matrix_knn[0,0] + conf_matrix_knn[1,1])/(conf_matrix_knn[0,0] +conf_matrix_knn[0,1]+conf_matrix_knn[1,0]+conf_matrix_knn[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn))\n",
    "\n",
    "print(y_pred_knn)\n",
    "\n",
    "print(conf_matrix_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best leaf_size: 1\n",
      "Best p: 1\n",
      "Best n_neighbors: 9\n",
      "Best Score: 0.76\n",
      "Best Hyperparameters: {'leaf_size': 1, 'n_neighbors': 9, 'p': 1}\n",
      "{'leaf_size': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], 'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'p': [1, 2]}\n",
      "83.33333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         6\n",
      "           1       0.75      1.00      0.86         6\n",
      "\n",
      "    accuracy                           0.83        12\n",
      "   macro avg       0.88      0.83      0.83        12\n",
      "weighted avg       0.88      0.83      0.83        12\n",
      "\n",
      "0.8333333333333334\n",
      "[1 1 0 0 1 1 0 0 1 1 1 1]\n",
      "[1 0 0 0 1 1 0 0 0 1 1 1]\n",
      "[[4 2]\n",
      " [0 6]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "########Hyperparameter tuning for KNN####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,20)) #neighbours must be < number of samples (22)\n",
    "p=[1,2]\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.33333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         6\n",
      "           1       0.75      1.00      0.86         6\n",
      "\n",
      "    accuracy                           0.83        12\n",
      "   macro avg       0.88      0.83      0.83        12\n",
      "weighted avg       0.88      0.83      0.83        12\n",
      "\n",
      "0.8333333333333334\n",
      "[1 1 0 0 1 1 0 0 1 1 1 1]\n",
      "[1 0 0 0 1 1 0 0 0 1 1 1]\n",
      "[[4 2]\n",
      " [0 6]]\n"
     ]
    }
   ],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors = 9, p = 1, leaf_size = 1)\n",
    "model_knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = model_knn.predict(X_test)\n",
    "\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "accuracy_knn = ((conf_matrix_knn[0,0] + conf_matrix_knn[1,1])/(conf_matrix_knn[0,0] +conf_matrix_knn[0,1]+conf_matrix_knn[1,0]+conf_matrix_knn[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn))\n",
    "\n",
    "print(y_pred_knn)\n",
    "print(y_test)\n",
    "print(conf_matrix_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Leave one out method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.649 (0.477)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      "Confusion Matrix for KNN using k-fold (leave one out)\n",
      "[[18  3]\n",
      " [10  6]]\n",
      "64.86486486486487\n"
     ]
    }
   ],
   "source": [
    "df_X = sc.fit_transform(df_X)\n",
    "k_fold = KFold(n_splits=37)\n",
    "#KNN\n",
    "model_knn_kfold = KNeighborsClassifier(n_neighbors = 9, p =1, leaf_size = 1)\n",
    "y_pred_kfold_knn = cross_val_predict(model_knn_kfold, df_X, df_Y, cv=k_fold)\n",
    "\n",
    "scores = cross_val_score(model_knn_kfold, df_X, df_Y, scoring='accuracy', cv=k_fold, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "\n",
    "print(df_Y)\n",
    "print(y_pred_kfold_knn)\n",
    "conf_matrix_knn_kfold = confusion_matrix(df_Y, y_pred_kfold_knn)\n",
    "print(\"Confusion Matrix for KNN using k-fold (leave one out)\")\n",
    "print(conf_matrix_knn_kfold)\n",
    "\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_kfold[0,0] + conf_matrix_knn_kfold[1,1])/(conf_matrix_knn_kfold[0,0] +conf_matrix_knn_kfold[0,1]+conf_matrix_knn_kfold[1,0]+conf_matrix_knn_kfold[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divided into 4 parts.\n",
      "    0\n",
      "2   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "10  0\n",
      "13  0\n",
      "19  0\n",
      "23  1\n",
      "27  1\n",
      "31  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "3   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "11  0\n",
      "12  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "14  0\n",
      "20  0\n",
      "26  1\n",
      "30  1\n",
      "32  1\n",
      "    0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "31  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "3   0\n",
      "11  0\n",
      "12  0\n",
      "18  0\n",
      "21  1\n",
      "22  1\n",
      "24  1\n",
      "33  1\n",
      "35  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "19  0\n",
      "20  0\n",
      "23  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "34  1\n",
      "36  1\n",
      "    0\n",
      "1   0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "25  1\n",
      "28  1\n",
      "29  1\n",
      "34  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "26  1\n",
      "27  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "35  1\n",
      "    0\n",
      "1   0\n",
      "5   0\n",
      "9   0\n",
      "14  0\n",
      "22  1\n",
      "23  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "34  1\n",
      "    0\n",
      "0   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "33  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "2   0\n",
      "12  0\n",
      "16  0\n",
      "18  0\n",
      "21  1\n",
      "24  1\n",
      "26  1\n",
      "33  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "17  0\n",
      "19  0\n",
      "20  0\n",
      "22  1\n",
      "23  1\n",
      "25  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "34  1\n",
      "35  1\n",
      "    0\n",
      "0   0\n",
      "3   0\n",
      "6   0\n",
      "8   0\n",
      "10  0\n",
      "11  0\n",
      "15  0\n",
      "19  0\n",
      "25  1\n",
      "    0\n",
      "1   0\n",
      "2   0\n",
      "4   0\n",
      "5   0\n",
      "7   0\n",
      "9   0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "4   0\n",
      "7   0\n",
      "13  0\n",
      "17  0\n",
      "20  0\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "35  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "5   0\n",
      "6   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "18  0\n",
      "19  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n",
      "36  1\n",
      "    0\n",
      "2   0\n",
      "8   0\n",
      "9   0\n",
      "11  0\n",
      "15  0\n",
      "21  1\n",
      "23  1\n",
      "27  1\n",
      "30  1\n",
      "34  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "10  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "22  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "28  1\n",
      "29  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "6   0\n",
      "7   0\n",
      "12  0\n",
      "14  0\n",
      "16  0\n",
      "22  1\n",
      "25  1\n",
      "29  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "13  0\n",
      "15  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "23  1\n",
      "24  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "10  0\n",
      "20  0\n",
      "24  1\n",
      "26  1\n",
      "28  1\n",
      "32  1\n",
      "33  1\n",
      "    0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "25  1\n",
      "27  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "13  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "31  1\n",
      "35  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n",
      "36  1\n",
      "    0\n",
      "3   0\n",
      "4   0\n",
      "7   0\n",
      "8   0\n",
      "14  0\n",
      "16  0\n",
      "17  0\n",
      "25  1\n",
      "31  1\n",
      "33  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "5   0\n",
      "6   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "15  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "32  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "5   0\n",
      "6   0\n",
      "13  0\n",
      "23  1\n",
      "24  1\n",
      "27  1\n",
      "29  1\n",
      "32  1\n",
      "35  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "25  1\n",
      "26  1\n",
      "28  1\n",
      "30  1\n",
      "31  1\n",
      "33  1\n",
      "34  1\n",
      "36  1\n",
      "    0\n",
      "1   0\n",
      "9   0\n",
      "11  0\n",
      "15  0\n",
      "18  0\n",
      "22  1\n",
      "26  1\n",
      "28  1\n",
      "30  1\n",
      "    0\n",
      "0   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "10  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "16  0\n",
      "17  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "27  1\n",
      "29  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "2   0\n",
      "10  0\n",
      "12  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "34  1\n",
      "36  1\n",
      "    0\n",
      "1   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "11  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "35  1\n",
      "    0\n",
      "3   0\n",
      "4   0\n",
      "6   0\n",
      "8   0\n",
      "10  0\n",
      "15  0\n",
      "16  0\n",
      "22  1\n",
      "34  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "5   0\n",
      "7   0\n",
      "9   0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "35  1\n",
      "    0\n",
      "0   0\n",
      "2   0\n",
      "7   0\n",
      "9   0\n",
      "17  0\n",
      "19  0\n",
      "20  0\n",
      "30  1\n",
      "32  1\n",
      "    0\n",
      "1   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "8   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "18  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "31  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "11  0\n",
      "13  0\n",
      "18  0\n",
      "21  1\n",
      "24  1\n",
      "26  1\n",
      "27  1\n",
      "31  1\n",
      "33  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "12  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "19  0\n",
      "20  0\n",
      "22  1\n",
      "23  1\n",
      "25  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "32  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "1   0\n",
      "5   0\n",
      "12  0\n",
      "14  0\n",
      "23  1\n",
      "25  1\n",
      "28  1\n",
      "29  1\n",
      "35  1\n",
      "    0\n",
      "0   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "13  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "24  1\n",
      "26  1\n",
      "27  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n",
      "36  1\n",
      "    0\n",
      "5   0\n",
      "7   0\n",
      "10  0\n",
      "11  0\n",
      "21  1\n",
      "26  1\n",
      "28  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "6   0\n",
      "8   0\n",
      "9   0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "27  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "36  1\n",
      "    0\n",
      "9   0\n",
      "12  0\n",
      "13  0\n",
      "15  0\n",
      "19  0\n",
      "20  0\n",
      "22  1\n",
      "24  1\n",
      "32  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "10  0\n",
      "11  0\n",
      "14  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "21  1\n",
      "23  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "2   0\n",
      "6   0\n",
      "8   0\n",
      "27  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "36  1\n",
      "    0\n",
      "1   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "7   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "28  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "    0\n",
      "1   0\n",
      "3   0\n",
      "4   0\n",
      "14  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "23  1\n",
      "25  1\n",
      "    0\n",
      "0   0\n",
      "2   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "15  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "24  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "16  0\n",
      "18  0\n",
      "20  0\n",
      "23  1\n",
      "28  1\n",
      "33  1\n",
      "    0\n",
      "1   0\n",
      "2   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "17  0\n",
      "19  0\n",
      "21  1\n",
      "22  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "1   0\n",
      "6   0\n",
      "7   0\n",
      "9   0\n",
      "10  0\n",
      "12  0\n",
      "27  1\n",
      "29  1\n",
      "31  1\n",
      "    0\n",
      "0   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "8   0\n",
      "11  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "28  1\n",
      "30  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "2   0\n",
      "11  0\n",
      "13  0\n",
      "15  0\n",
      "21  1\n",
      "24  1\n",
      "32  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "12  0\n",
      "14  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "22  1\n",
      "23  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "33  1\n",
      "34  1\n",
      "    0\n",
      "8   0\n",
      "14  0\n",
      "17  0\n",
      "19  0\n",
      "22  1\n",
      "25  1\n",
      "26  1\n",
      "30  1\n",
      "34  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "15  0\n",
      "16  0\n",
      "18  0\n",
      "20  0\n",
      "21  1\n",
      "23  1\n",
      "24  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "4   0\n",
      "12  0\n",
      "13  0\n",
      "19  0\n",
      "20  0\n",
      "26  1\n",
      "30  1\n",
      "31  1\n",
      "34  1\n",
      "    0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "32  1\n",
      "33  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "1   0\n",
      "3   0\n",
      "6   0\n",
      "8   0\n",
      "10  0\n",
      "17  0\n",
      "21  1\n",
      "28  1\n",
      "32  1\n",
      "    0\n",
      "0   0\n",
      "2   0\n",
      "4   0\n",
      "5   0\n",
      "7   0\n",
      "9   0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "2   0\n",
      "7   0\n",
      "9   0\n",
      "11  0\n",
      "18  0\n",
      "23  1\n",
      "25  1\n",
      "27  1\n",
      "33  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "8   0\n",
      "10  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "24  1\n",
      "26  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "5   0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "22  1\n",
      "24  1\n",
      "29  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "23  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "10  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "21  1\n",
      "26  1\n",
      "    0\n",
      "0   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "11  0\n",
      "12  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "8   0\n",
      "18  0\n",
      "24  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "34  1\n",
      "35  1\n",
      "    0\n",
      "0   0\n",
      "5   0\n",
      "7   0\n",
      "9   0\n",
      "11  0\n",
      "19  0\n",
      "25  1\n",
      "27  1\n",
      "28  1\n",
      "    0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "6   0\n",
      "8   0\n",
      "10  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "26  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "6   0\n",
      "12  0\n",
      "16  0\n",
      "17  0\n",
      "20  0\n",
      "22  1\n",
      "23  1\n",
      "34  1\n",
      "35  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "18  0\n",
      "19  0\n",
      "21  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "36  1\n",
      "    0\n",
      "1   0\n",
      "2   0\n",
      "4   0\n",
      "7   0\n",
      "8   0\n",
      "15  0\n",
      "22  1\n",
      "24  1\n",
      "25  1\n",
      "27  1\n",
      "    0\n",
      "0   0\n",
      "3   0\n",
      "5   0\n",
      "6   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "23  1\n",
      "26  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "12  0\n",
      "13  0\n",
      "18  0\n",
      "26  1\n",
      "28  1\n",
      "30  1\n",
      "31  1\n",
      "33  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "27  1\n",
      "29  1\n",
      "32  1\n",
      "34  1\n",
      "35  1\n",
      "    0\n",
      "0   0\n",
      "3   0\n",
      "6   0\n",
      "9   0\n",
      "11  0\n",
      "16  0\n",
      "19  0\n",
      "21  1\n",
      "34  1\n",
      "    0\n",
      "1   0\n",
      "2   0\n",
      "4   0\n",
      "5   0\n",
      "7   0\n",
      "8   0\n",
      "10  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "17  0\n",
      "18  0\n",
      "20  0\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "5   0\n",
      "10  0\n",
      "14  0\n",
      "17  0\n",
      "20  0\n",
      "23  1\n",
      "29  1\n",
      "32  1\n",
      "35  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "15  0\n",
      "16  0\n",
      "18  0\n",
      "19  0\n",
      "21  1\n",
      "22  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "30  1\n",
      "31  1\n",
      "33  1\n",
      "34  1\n",
      "36  1\n",
      "    0\n",
      "2   0\n",
      "9   0\n",
      "10  0\n",
      "12  0\n",
      "14  0\n",
      "22  1\n",
      "26  1\n",
      "28  1\n",
      "32  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "3   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "11  0\n",
      "13  0\n",
      "15  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "27  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "    0\n",
      "1   0\n",
      "5   0\n",
      "17  0\n",
      "19  0\n",
      "20  0\n",
      "24  1\n",
      "30  1\n",
      "34  1\n",
      "35  1\n",
      "    0\n",
      "0   0\n",
      "2   0\n",
      "3   0\n",
      "4   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "16  0\n",
      "18  0\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "36  1\n",
      "    0\n",
      "3   0\n",
      "16  0\n",
      "18  0\n",
      "21  1\n",
      "25  1\n",
      "27  1\n",
      "29  1\n",
      "31  1\n",
      "33  1\n",
      "    0\n",
      "0   0\n",
      "1   0\n",
      "2   0\n",
      "4   0\n",
      "5   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "9   0\n",
      "10  0\n",
      "11  0\n",
      "12  0\n",
      "13  0\n",
      "14  0\n",
      "15  0\n",
      "17  0\n",
      "19  0\n",
      "20  0\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "26  1\n",
      "28  1\n",
      "30  1\n",
      "32  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "    0\n",
      "0   0\n",
      "4   0\n",
      "6   0\n",
      "7   0\n",
      "8   0\n",
      "11  0\n",
      "13  0\n",
      "15  0\n",
      "23  1\n",
      "    0\n",
      "1   0\n",
      "2   0\n",
      "3   0\n",
      "5   0\n",
      "9   0\n",
      "10  0\n",
      "12  0\n",
      "14  0\n",
      "16  0\n",
      "17  0\n",
      "18  0\n",
      "19  0\n",
      "20  0\n",
      "21  1\n",
      "22  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      "30  1\n",
      "31  1\n",
      "32  1\n",
      "33  1\n",
      "34  1\n",
      "35  1\n",
      "36  1\n",
      "KNN Kfold Evaluation for MDVR-KCL Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loops</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>59.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50.0</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>48.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>40.0</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>65.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>70.0</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>61.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>50.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>65.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>64.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>59.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>80.0</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>64.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>50.0</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>51.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>60.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loops  fold 1     fold 2     fold 3     fold 4  mean accuracy\n",
       "0       1    60.0  55.555556  66.666667  55.555556      59.444444\n",
       "1       2    50.0  44.444444  44.444444  55.555556      48.611111\n",
       "2       3    40.0  77.777778  66.666667  77.777778      65.555556\n",
       "3       4    70.0  33.333333  77.777778  66.666667      61.944444\n",
       "4       5    80.0  44.444444  33.333333  44.444444      50.555556\n",
       "5       6    50.0  66.666667  55.555556  88.888889      65.277778\n",
       "6       7    80.0  66.666667  55.555556  55.555556      64.444444\n",
       "7       8    60.0  55.555556  55.555556  66.666667      59.444444\n",
       "8       9    80.0  44.444444  66.666667  66.666667      64.444444\n",
       "9      10    50.0  44.444444  44.444444  66.666667      51.388889\n",
       "10     11    60.0  66.666667  55.555556  77.777778      65.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_list = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row.append(i)\n",
    "    total = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        print(Ytest_kfold)\n",
    "        print(Ytrain_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling\n",
    "        model_knn_new = KNeighborsClassifier(n_neighbors = 9, p =1, leaf_size = 1)\n",
    "        model_knn_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "        y_pred_knn_new = model_knn_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_knn_kfold = confusion_matrix(Ytest_kfold, y_pred_knn_new)\n",
    "\n",
    "        accuracy_knn_kfold = ((conf_matrix_knn_kfold[0,0] + conf_matrix_knn_kfold[1,1])/(conf_matrix_knn_kfold[0,0] +conf_matrix_knn_kfold[0,1]+conf_matrix_knn_kfold[1,0]+conf_matrix_knn_kfold[1,1]))*100\n",
    "\n",
    "        #print(\"Confusion Matrix:\\n \", conf_matrix_knn_kfold)\n",
    "        #print(\"Accuracy \", accuracy_knn_kfold)\n",
    "\n",
    "        row.append(accuracy_knn_kfold)\n",
    "        total += accuracy_knn_kfold\n",
    "    average = row.append(total/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_list.append(row)\n",
    "    \n",
    "\n",
    "k_list = pd.DataFrame(k_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"KNN Kfold Evaluation for MDVR-KCL Dataset\")\n",
    "k_list\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#print(df.sample(n=7))\n",
    "#print(df)\n",
    "\n",
    "#x = df.take(np.random.permutation(len(df))[:4])\n",
    "#x= df.sample(n=7)\n",
    "#print(x)\n",
    "#print(df.drop(x))\n",
    "\n",
    "#drop_indices = np.random.choice(df.index, 4, replace=False)\n",
    "#df_subset = df.drop(drop_indices)\n",
    "#print(drop_indices)\n",
    "#print(df_subset)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************Decision Tree Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         6\n",
      "           1       0.67      1.00      0.80         6\n",
      "\n",
      "    accuracy                           0.75        12\n",
      "   macro avg       0.83      0.75      0.73        12\n",
      "weighted avg       0.83      0.75      0.73        12\n",
      "\n",
      "0.75\n",
      "[1 1 0 0 1 1 0 1 1 1 1 1]\n",
      "[[3 3]\n",
      " [0 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Text(334.8, 684.9359999999999, 'X[0] <= 0.279\\ngini = 0.48\\nsamples = 25\\nvalue = [15, 10]'),\n",
       " Text(167.4, 532.728, 'gini = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(502.20000000000005, 532.728, 'X[6] <= 0.254\\ngini = 0.408\\nsamples = 21\\nvalue = [15, 6]'),\n",
       " Text(334.8, 380.52, 'gini = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(669.6, 380.52, 'X[3] <= 0.376\\ngini = 0.48\\nsamples = 15\\nvalue = [9, 6]'),\n",
       " Text(334.8, 228.312, 'X[0] <= 0.668\\ngini = 0.408\\nsamples = 7\\nvalue = [2, 5]'),\n",
       " Text(167.4, 76.10399999999993, 'gini = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(502.20000000000005, 76.10399999999993, 'gini = 0.0\\nsamples = 5\\nvalue = [0, 5]'),\n",
       " Text(1004.4000000000001, 228.312, 'X[2] <= 0.056\\ngini = 0.219\\nsamples = 8\\nvalue = [7, 1]'),\n",
       " Text(837.0, 76.10399999999993, 'gini = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(1171.8, 76.10399999999993, 'gini = 0.0\\nsamples = 7\\nvalue = [7, 0]')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x1008 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#Decision Tree\n",
    "import matplotlib.pyplot as plt\n",
    "model_dt = tree.DecisionTreeClassifier()\n",
    "model_dt = model_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = model_dt.predict(X_test)\n",
    "\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "accuracy_knn = ((conf_matrix_knn[0,0] + conf_matrix_knn[1,1])/(conf_matrix_knn[0,0] +conf_matrix_knn[0,1]+conf_matrix_knn[1,0]+conf_matrix_knn[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn))\n",
    "\n",
    "print(y_pred_knn)\n",
    "\n",
    "print(conf_matrix_knn)\n",
    "\n",
    "plt.figure(figsize=(24,14))\n",
    "tree.plot_tree(model_dt, filled=True, fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.6\n",
      "Best Hyperparameters: {'criterion': 'gini', 'max_depth': 1, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "{'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9], 'min_samples_leaf': [1, 2, 3, 4], 'criterion': ['gini', 'entropy']}\n",
      "75.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73         6\n",
      "           1       0.71      0.83      0.77         6\n",
      "\n",
      "    accuracy                           0.75        12\n",
      "   macro avg       0.76      0.75      0.75        12\n",
      "weighted avg       0.76      0.75      0.75        12\n",
      "\n",
      "0.7500000000000002\n",
      "[1 1 0 0 1 1 0 0 1 0 1 1]\n",
      "[1 0 0 0 1 1 0 0 0 1 1 1]\n",
      "[[4 2]\n",
      " [1 5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "########Hyperparameter tuning for KNN####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "#n_components = list(range(1,X.shape[1]+1,1))\n",
    "max_depth = list(range(1,10))\n",
    "min_samples_split = list(range(2,10)) #neighbours must be < number of samples (22)\n",
    "min_samples_leaf = list(range(1,5))\n",
    "criterion=['gini','entropy']\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, criterion = criterion)\n",
    "#Create new KNN object\n",
    "dt_2 = tree.DecisionTreeClassifier()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(dt_2, hyperparameters, refit=True)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "#print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "#print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "#print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model (Decision Tree)\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2]\n",
      " [1 5]]\n",
      "75.0\n"
     ]
    }
   ],
   "source": [
    "#model_dt = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "model_dt = tree.DecisionTreeClassifier(max_depth = 1, criterion='gini',min_samples_leaf=1, min_samples_split=2 )\n",
    "model_dt = model_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_dt_2 = confusion_matrix(y_test, y_pred_dt_2)\n",
    "\n",
    "accuracy_dt_2 = ((conf_matrix_dt_2[0,0] + conf_matrix_dt_2[1,1])/(conf_matrix_dt_2[0,0] +conf_matrix_dt_2[0,1]+conf_matrix_dt_2[1,0]+conf_matrix_dt_2[1,1]))*100\n",
    "\n",
    "print(conf_matrix_dt_2)\n",
    "print(accuracy_dt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Leave one out method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.568 (0.495)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0]\n",
      "Confusion Matrix for Dt using k-fold (leave one out)\n",
      "[[17  4]\n",
      " [11  5]]\n",
      "59.45945945945946\n"
     ]
    }
   ],
   "source": [
    "df_X = sc.fit_transform(df_X)\n",
    "k_fold = KFold(n_splits=37)\n",
    "#KNN\n",
    "#model_knn_kfold = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "model_dt = tree.DecisionTreeClassifier(max_depth = 1, criterion='gini',min_samples_leaf=1, min_samples_split=2 )\n",
    "y_pred_kfold_knn = cross_val_predict(model_dt, df_X, df_Y, cv=k_fold)\n",
    "scores = cross_val_score(model_dt, df_X, df_Y, scoring='accuracy', cv=k_fold, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "\n",
    "print(df_Y)\n",
    "print(y_pred_kfold_knn)\n",
    "conf_matrix_knn_kfold = confusion_matrix(df_Y, y_pred_kfold_knn)\n",
    "print(\"Confusion Matrix for Dt using k-fold (leave one out)\")\n",
    "print(conf_matrix_knn_kfold)\n",
    "\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_kfold[0,0] + conf_matrix_knn_kfold[1,1])/(conf_matrix_knn_kfold[0,0] +conf_matrix_knn_kfold[0,1]+conf_matrix_knn_kfold[1,0]+conf_matrix_knn_kfold[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divided into 4 parts.\n",
      "Decsion Tree Kfold Evaluation for MDVR-KCL Dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loops</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>51.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>53.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>62.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>58.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>70.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>61.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>50.0</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>56.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>64.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>60.0</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>56.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>70.0</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>45.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>50.0</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>59.722222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loops  fold 1     fold 2     fold 3     fold 4  mean accuracy\n",
       "0       1    60.0  66.666667  55.555556  77.777778      65.000000\n",
       "1       2    40.0  44.444444  66.666667  55.555556      51.666667\n",
       "2       3    60.0  55.555556  44.444444  55.555556      53.888889\n",
       "3       4    60.0  55.555556  66.666667  66.666667      62.222222\n",
       "4       5    80.0  55.555556  33.333333  66.666667      58.888889\n",
       "5       6    70.0  66.666667  44.444444  66.666667      61.944444\n",
       "6       7    50.0  55.555556  66.666667  55.555556      56.944444\n",
       "7       8    70.0  55.555556  55.555556  77.777778      64.722222\n",
       "8       9    60.0  55.555556  66.666667  44.444444      56.666667\n",
       "9      10    70.0  22.222222  33.333333  55.555556      45.277778\n",
       "10     11    50.0  77.777778  44.444444  66.666667      59.722222"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_list = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row.append(i)\n",
    "    total = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling\n",
    "        #model_knn_new = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "        model_dt_new = tree.DecisionTreeClassifier(max_depth = 1, criterion='gini',min_samples_leaf=1, min_samples_split=2 )\n",
    "        model_dt_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "        y_pred_dt_new = model_dt_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_dt_kfold = confusion_matrix(Ytest_kfold, y_pred_dt_new)\n",
    "\n",
    "        accuracy_dt_kfold = ((conf_matrix_dt_kfold[0,0] + conf_matrix_dt_kfold[1,1])/(conf_matrix_dt_kfold[0,0] +conf_matrix_dt_kfold[0,1]+conf_matrix_dt_kfold[1,0]+conf_matrix_dt_kfold[1,1]))*100\n",
    "\n",
    "        #print(\"Confusion Matrix:\\n \", conf_matrix_knn_kfold)\n",
    "        #print(\"Accuracy \", accuracy_knn_kfold)\n",
    "\n",
    "        row.append(accuracy_dt_kfold)\n",
    "        total += accuracy_dt_kfold\n",
    "    average = row.append(total/parts)\n",
    "        \n",
    "    #print(row)\n",
    "    k_list.append(row)\n",
    "    \n",
    "k_list = pd.DataFrame(k_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"Decsion Tree Kfold Evaluation for MDVR-KCL Dataset\")\n",
    "k_list\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#print(df.sample(n=7))\n",
    "#print(df)\n",
    "\n",
    "#x = df.take(np.random.permutation(len(df))[:4])\n",
    "#x= df.sample(n=7)\n",
    "#print(x)\n",
    "#print(df.drop(x))\n",
    "\n",
    "#drop_indices = np.random.choice(df.index, 4, replace=False)\n",
    "#df_subset = df.drop(drop_indices)\n",
    "#print(drop_indices)\n",
    "#print(df_subset)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************SVM Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.67      0.57         6\n",
      "           1       0.50      0.33      0.40         6\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.50      0.50      0.49        12\n",
      "weighted avg       0.50      0.50      0.49        12\n",
      "\n",
      "0.5\n",
      "[0 1 0 0 1 0 0 0 1 0 1 0]\n",
      "[[4 2]\n",
      " [4 2]]\n"
     ]
    }
   ],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#Decision Tree\n",
    "import matplotlib.pyplot as plt\n",
    "model_svm = svm.SVC()\n",
    "model_svm = model_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = model_dt.predict(X_test)\n",
    "\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "\n",
    "accuracy_svm = ((conf_matrix_svm[0,0] + conf_matrix_svm[1,1])/(conf_matrix_svm[0,0] +conf_matrix_svm[0,1]+conf_matrix_svm[1,0]+conf_matrix_svm[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_svm)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_svm))\n",
    "\n",
    "print(y_pred_svm)\n",
    "\n",
    "print(conf_matrix_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.68\n",
      "Best Hyperparameters: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "{'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']}\n",
      "66.66666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67         6\n",
      "           1       0.67      0.67      0.67         6\n",
      "\n",
      "    accuracy                           0.67        12\n",
      "   macro avg       0.67      0.67      0.67        12\n",
      "weighted avg       0.67      0.67      0.67        12\n",
      "\n",
      "0.6666666666666667\n",
      "[1 1 0 0 1 1 0 0 1 0 1 0]\n",
      "[1 0 0 0 1 1 0 0 0 1 1 1]\n",
      "[[4 2]\n",
      " [2 4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "########Hyperparameter tuning for SVM####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "#n_components = list(range(1,X.shape[1]+1,1))\n",
    "C = [0.1, 1, 10, 100, 1000]\n",
    "gamma = [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "kernel = ['rbf']\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(C=C, gamma=gamma, kernel=kernel)\n",
    "#Create new KNN object\n",
    "svm2 = svm.SVC()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(svm2, hyperparameters, refit=True)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "#print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "#print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "#print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model (SVM)\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2]\n",
      " [2 4]]\n",
      "66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "model_svm = svm.SVC(C = 10, gamma=0.1,kernel='rbf')\n",
    "model_svm = model_svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_svm_2 = confusion_matrix(y_test, y_pred_svm_2)\n",
    "\n",
    "accuracy_svm_2 = ((conf_matrix_svm_2[0,0] + conf_matrix_svm_2[1,1])/(conf_matrix_svm_2[0,0] +conf_matrix_svm_2[0,1]+conf_matrix_svm_2[1,0]+conf_matrix_svm_2[1,1]))*100\n",
    "\n",
    "print(conf_matrix_svm_2)\n",
    "print(accuracy_svm_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (SVM) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Leave one out method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.757 (0.429)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0]\n",
      "Confusion Matrix for Dt using k-fold (leave one out)\n",
      "[[17  4]\n",
      " [ 5 11]]\n",
      "75.67567567567568\n"
     ]
    }
   ],
   "source": [
    "df_X = sc.fit_transform(df_X)\n",
    "k_fold = KFold(n_splits=37)\n",
    "#KNN\n",
    "#model_knn_kfold = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "model_svm = svm.SVC(C = 10, gamma=0.1,kernel='rbf')\n",
    "y_pred_kfold_svm = cross_val_predict(model_svm, df_X, df_Y, cv=k_fold)\n",
    "scores = cross_val_score(model_svm, df_X, df_Y, scoring='accuracy', cv=k_fold, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "\n",
    "print(df_Y)\n",
    "print(y_pred_kfold_knn)\n",
    "conf_matrix_svm_kfold = confusion_matrix(df_Y, y_pred_kfold_svm)\n",
    "print(\"Confusion Matrix for Dt using k-fold (leave one out)\")\n",
    "print(conf_matrix_svm_kfold)\n",
    "\n",
    "\n",
    "accuracy_svm_2 = ((conf_matrix_svm_kfold[0,0] + conf_matrix_svm_kfold[1,1])/(conf_matrix_svm_kfold[0,0] +conf_matrix_svm_kfold[0,1]+conf_matrix_svm_kfold[1,0]+conf_matrix_svm_kfold[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_svm_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divided into 4 parts.\n",
      "SVM Kfold Evaluation for MDVR-KCL Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loops</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>70.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>70.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>75.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>70.0</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>75.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>70.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>64.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>60.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>67.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>60.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>67.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>75.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>80.0</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>72.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>70.0</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>70.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>80.0</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>75.555556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loops  fold 1     fold 2      fold 3     fold 4  mean accuracy\n",
       "0       1    70.0  66.666667   77.777778  66.666667      70.277778\n",
       "1       2    60.0  77.777778   77.777778  66.666667      70.555556\n",
       "2       3    80.0  66.666667   77.777778  77.777778      75.555556\n",
       "3       4    70.0  77.777778   77.777778  77.777778      75.833333\n",
       "4       5    70.0  66.666667   88.888889  33.333333      64.722222\n",
       "5       6    60.0  66.666667   55.555556  88.888889      67.777778\n",
       "6       7    60.0  66.666667   66.666667  77.777778      67.777778\n",
       "7       8    80.0  77.777778   66.666667  77.777778      75.555556\n",
       "8       9    80.0  44.444444   77.777778  88.888889      72.777778\n",
       "9      10    70.0  44.444444   88.888889  77.777778      70.277778\n",
       "10     11    80.0  44.444444  100.000000  77.777778      75.555556"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_list = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row.append(i)\n",
    "    total = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "\n",
    "        #modelling\n",
    "        #model_knn_new = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "        model_svm_new = svm.SVC(C = 10, gamma=0.1,kernel='rbf')\n",
    "        model_svm_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "        y_pred_svm_new = model_svm_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_svm_kfold = confusion_matrix(Ytest_kfold, y_pred_svm_new)\n",
    "\n",
    "        accuracy_svm_kfold = ((conf_matrix_svm_kfold[0,0] + conf_matrix_svm_kfold[1,1])/(conf_matrix_svm_kfold[0,0] +conf_matrix_svm_kfold[0,1]+conf_matrix_svm_kfold[1,0]+conf_matrix_svm_kfold[1,1]))*100\n",
    "\n",
    "        #print(\"Confusion Matrix:\\n \", conf_matrix_knn_kfold)\n",
    "        #print(\"Accuracy \", accuracy_knn_kfold)\n",
    "\n",
    "        row.append(accuracy_svm_kfold)\n",
    "        total += accuracy_svm_kfold\n",
    "    average = row.append(total/parts)\n",
    "    #print(row)\n",
    "    k_list.append(row)\n",
    "    \n",
    "k_list = pd.DataFrame(k_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"SVM Kfold Evaluation for MDVR-KCL Dataset\")\n",
    "k_list\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#print(df.sample(n=7))\n",
    "#print(df)\n",
    "\n",
    "#x = df.take(np.random.permutation(len(df))[:4])\n",
    "#x= df.sample(n=7)\n",
    "#print(x)\n",
    "#print(df.drop(x))\n",
    "\n",
    "#drop_indices = np.random.choice(df.index, 4, replace=False)\n",
    "#df_subset = df.drop(drop_indices)\n",
    "#print(drop_indices)\n",
    "#print(df_subset)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************Naive Bayes Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.66666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.33      0.50         6\n",
      "           1       0.60      1.00      0.75         6\n",
      "\n",
      "    accuracy                           0.67        12\n",
      "   macro avg       0.80      0.67      0.62        12\n",
      "weighted avg       0.80      0.67      0.62        12\n",
      "\n",
      "0.6666666666666667\n",
      "[1 1 0 1 1 1 0 1 1 1 1 1]\n",
      "[[2 4]\n",
      " [0 6]]\n"
     ]
    }
   ],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#NB\n",
    "model_nb = GaussianNB()\n",
    "model_nb = model_nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_nb = model_nb.predict(X_test)\n",
    "\n",
    "conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "\n",
    "accuracy_nb = ((conf_matrix_nb[0,0] + conf_matrix_nb[1,1])/(conf_matrix_nb[0,0] +conf_matrix_nb[0,1]+conf_matrix_nb[1,0]+conf_matrix_nb[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_nb)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_nb))\n",
    "\n",
    "print(y_pred_nb)\n",
    "\n",
    "print(conf_matrix_nb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB doesnt have important parameters to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (Naive Bayes) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Leave one out method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.757 (0.429)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 0]\n",
      "Confusion Matrix for Dt using k-fold (leave one out)\n",
      "[[17  4]\n",
      " [ 5 11]]\n",
      "75.67567567567568\n"
     ]
    }
   ],
   "source": [
    "df_X = sc.fit_transform(df_X)\n",
    "k_fold = KFold(n_splits=37)\n",
    "#KNN\n",
    "#model_knn_kfold = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "model_nb_2 = GaussianNB()\n",
    "y_pred_kfold_nb = cross_val_predict(model_nb_2, df_X, df_Y, cv=k_fold)\n",
    "scores = cross_val_score(model_nb_2, df_X, df_Y, scoring='accuracy', cv=k_fold, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "\n",
    "print(df_Y)\n",
    "print(y_pred_kfold_nb)\n",
    "conf_matrix_nb_kfold = confusion_matrix(df_Y, y_pred_kfold_nb)\n",
    "print(\"Confusion Matrix for Dt using k-fold (leave one out)\")\n",
    "print(conf_matrix_nb_kfold)\n",
    "\n",
    "\n",
    "accuracy_nb_2 = ((conf_matrix_nb_kfold[0,0] + conf_matrix_nb_kfold[1,1])/(conf_matrix_nb_kfold[0,0] +conf_matrix_nb_kfold[0,1]+conf_matrix_nb_kfold[1,0]+conf_matrix_nb_kfold[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_nb_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divided into 4 parts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Kfold Evaluation for MDVR-KCL Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loops</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>72.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>80.0</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>75.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>73.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>75.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>60.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>73.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>73.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>72.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>72.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>70.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>73.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>90.0</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>66.944444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loops  fold 1      fold 2     fold 3     fold 4  mean accuracy\n",
       "0       1    80.0   77.777778  44.444444  88.888889      72.777778\n",
       "1       2    80.0   55.555556  66.666667  77.777778      70.000000\n",
       "2       3    70.0   66.666667  88.888889  77.777778      75.833333\n",
       "3       4    50.0   88.888889  66.666667  88.888889      73.611111\n",
       "4       5    80.0   66.666667  88.888889  66.666667      75.555556\n",
       "5       6    60.0   66.666667  88.888889  77.777778      73.333333\n",
       "6       7    70.0   55.555556  77.777778  88.888889      73.055556\n",
       "7       8    80.0   66.666667  66.666667  77.777778      72.777778\n",
       "8       9    80.0  100.000000  55.555556  55.555556      72.777778\n",
       "9      10    70.0   66.666667  66.666667  88.888889      73.055556\n",
       "10     11    90.0   77.777778  44.444444  55.555556      66.944444"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_list = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row.append(i)\n",
    "    total = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "        \n",
    "\n",
    "        #modelling\n",
    "        #model_knn_new = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "        model_nb_new = GaussianNB()\n",
    "        model_nb_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "\n",
    "        y_pred_nb_new = model_nb_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_nb_kfold = confusion_matrix(Ytest_kfold, y_pred_nb_new)\n",
    "\n",
    "        accuracy_nb_kfold = ((conf_matrix_nb_kfold[0,0] + conf_matrix_nb_kfold[1,1])/(conf_matrix_nb_kfold[0,0] +conf_matrix_nb_kfold[0,1]+conf_matrix_nb_kfold[1,0]+conf_matrix_nb_kfold[1,1]))*100\n",
    "\n",
    "        #print(\"Confusion Matrix:\\n \", conf_matrix_knn_kfold)\n",
    "        #print(\"Accuracy \", accuracy_knn_kfold)\n",
    "\n",
    "        row.append(accuracy_nb_kfold)\n",
    "        total += accuracy_nb_kfold\n",
    "    average = row.append(total/parts)\n",
    "    #print(row)\n",
    "    k_list.append(row)\n",
    "    \n",
    "k_list = pd.DataFrame(k_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"SVM Kfold Evaluation for MDVR-KCL Dataset\")\n",
    "k_list\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#print(df.sample(n=7))\n",
    "#print(df)\n",
    "\n",
    "#x = df.take(np.random.permutation(len(df))[:4])\n",
    "#x= df.sample(n=7)\n",
    "#print(x)\n",
    "#print(df.drop(x))\n",
    "\n",
    "#drop_indices = np.random.choice(df.index, 4, replace=False)\n",
    "#df_subset = df.drop(drop_indices)\n",
    "#print(drop_indices)\n",
    "#print(df_subset)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# *****************Logistic Regression Experiments***************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "### without tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73         6\n",
      "           1       0.71      0.83      0.77         6\n",
      "\n",
      "    accuracy                           0.75        12\n",
      "   macro avg       0.76      0.75      0.75        12\n",
      "weighted avg       0.76      0.75      0.75        12\n",
      "\n",
      "0.7500000000000002\n",
      "[1 1 0 0 1 1 0 0 1 1 1 0]\n",
      "[[4 2]\n",
      " [1 5]]\n"
     ]
    }
   ],
   "source": [
    "###### KNNN ###########\n",
    "# Fit classifier to the Training set\n",
    "#NB\n",
    "\n",
    "\n",
    "model_lr = LogisticRegression(random_state=0)\n",
    "model_lr = model_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "\n",
    "conf_matrix_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "accuracy_lr = ((conf_matrix_lr[0,0] + conf_matrix_lr[1,1])/(conf_matrix_lr[0,0] +conf_matrix_lr[0,1]+conf_matrix_lr[1,0]+conf_matrix_lr[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_lr)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_lr))\n",
    "\n",
    "print(y_pred_lr)\n",
    "\n",
    "print(conf_matrix_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7066666666666667\n",
      "Best Hyperparameters: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "{'solver': ['newton-cg', 'lbfgs', 'liblinear'], 'penalty': ['l2'], 'C': [100, 10, 1.0, 0.1, 0.01]}\n",
      "75.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73         6\n",
      "           1       0.71      0.83      0.77         6\n",
      "\n",
      "    accuracy                           0.75        12\n",
      "   macro avg       0.76      0.75      0.75        12\n",
      "weighted avg       0.76      0.75      0.75        12\n",
      "\n",
      "0.7500000000000002\n",
      "[1 1 0 0 1 1 0 0 1 1 1 0]\n",
      "[1 0 0 0 1 1 0 0 0 1 1 1]\n",
      "[[4 2]\n",
      " [1 5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "########Hyperparameter tuning for SVM####################\n",
    "#List Hyperparameters that we want to tune.\n",
    "#n_components = list(range(1,X.shape[1]+1,1))\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "#Create new LR object\n",
    "model_lr2 = LogisticRegression()\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(model_lr2, hyperparameters, cv=cv)\n",
    "\n",
    "#clf = RandomizedSearchCV(knn_2, hyperparameters, n_iter=500, cv=8, scoring=\"recall\")\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "#Print The value of best Hyperparameters\n",
    "#print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "#print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "#print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "print('Best Score: %s' % best_model.best_score_)\n",
    "print('Best Hyperparameters: %s' % best_model.best_params_)\n",
    "print(hyperparameters)\n",
    "y_pred_knn_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_knn_2 = confusion_matrix(y_test, y_pred_knn_2)\n",
    "\n",
    "accuracy_knn_2 = ((conf_matrix_knn_2[0,0] + conf_matrix_knn_2[1,1])/(conf_matrix_knn_2[0,0] +conf_matrix_knn_2[0,1]+conf_matrix_knn_2[1,0]+conf_matrix_knn_2[1,1]))*100\n",
    "\n",
    "\n",
    "print(accuracy_knn_2)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred_knn_2))\n",
    "#Checking performance our model with ROC Score.\n",
    "print(roc_auc_score(y_test, y_pred_knn_2))\n",
    "\n",
    "print(y_pred_knn_2)\n",
    "print(y_test)\n",
    "\n",
    "print(conf_matrix_knn_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model (LR)\n",
    "### using the optimal parameters gotten above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 2]\n",
      " [1 5]]\n",
      "75.0\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(C= 10, penalty='l2',solver= 'newton-cg')\n",
    "model_lr = model_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr_2 = best_model.predict(X_test)\n",
    "\n",
    "conf_matrix_lr_2 = confusion_matrix(y_test, y_pred_lr_2)\n",
    "\n",
    "accuracy_lr_2 = ((conf_matrix_lr_2[0,0] + conf_matrix_lr_2[1,1])/(conf_matrix_lr_2[0,0] +conf_matrix_lr_2[0,1]+conf_matrix_lr_2[1,0]+conf_matrix_lr_2[1,1]))*100\n",
    "\n",
    "print(conf_matrix_lr_2)\n",
    "print(accuracy_lr_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (LR) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold Evaluation using optimal paramaters. (k =4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divided into 4 parts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Aeesha\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Kfold Evaluation for MDVR-KCL Dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loops</th>\n",
       "      <th>fold 1</th>\n",
       "      <th>fold 2</th>\n",
       "      <th>fold 3</th>\n",
       "      <th>fold 4</th>\n",
       "      <th>mean accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>62.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>72.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>70.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>67.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>67.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>80.0</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>67.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>60.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>70.0</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>67.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>61.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>50.0</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>68.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loops  fold 1     fold 2     fold 3      fold 4  mean accuracy\n",
       "0       1    50.0  77.777778  44.444444   77.777778      62.500000\n",
       "1       2    80.0  66.666667  77.777778   66.666667      72.777778\n",
       "2       3    70.0  44.444444  66.666667  100.000000      70.277778\n",
       "3       4    70.0  55.555556  88.888889   55.555556      67.500000\n",
       "4       5    60.0  88.888889  66.666667   55.555556      67.777778\n",
       "5       6    80.0  55.555556  66.666667   66.666667      67.222222\n",
       "6       7    60.0  66.666667  66.666667   66.666667      65.000000\n",
       "7       8    70.0  77.777778  44.444444   77.777778      67.500000\n",
       "8       9    80.0  77.777778  55.555556   33.333333      61.666667\n",
       "9      10    50.0  55.555556  88.888889   77.777778      68.055556\n",
       "10     11    80.0  66.666667  66.666667   66.666667      70.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_kfold = shuffle(df)\n",
    "#df_kfold.reset_index(inplace=True, drop=True)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "df_kfold = df\n",
    "df_Xnew = df_kfold.iloc[:, :-1].values\n",
    "df_Ynew = df_kfold.iloc[:,-1].values\n",
    "\n",
    "\n",
    "X_kfold = pd.DataFrame(df_Xnew)\n",
    "y_kfold = pd.DataFrame(df_Ynew)\n",
    "#print(X_kfold)\n",
    "#print(y_kfold)\n",
    "\n",
    "parts = 4\n",
    "kfold = KFold(parts, True, None) \n",
    "\n",
    "# splits into 5 groups \n",
    "print(\"Divided into %s parts.\" %parts)\n",
    "\n",
    "k_list = []\n",
    "\n",
    "for i in range (1,12):\n",
    "    row = []\n",
    "    row.append(i)\n",
    "    total = 0\n",
    "    #print(\"Loop\", i)\n",
    "    for train, test in kfold.split(X_kfold,y_kfold):\n",
    "        #print('\\ntrain: %s, test: %s' % (train, test))\n",
    "        Xtrain_kfold = X_kfold.iloc[train, :]\n",
    "        Ytrain_kfold = y_kfold.iloc[train, :]\n",
    "        Xtest_kfold = X_kfold.iloc[test, :]\n",
    "        Ytest_kfold = y_kfold.iloc[test, :]\n",
    "        #print(Xtest_kfold)\n",
    "        #print(Ytest_kfold)\n",
    "\n",
    "        Xtrain_kfold = sc.fit_transform(Xtrain_kfold)\n",
    "        Xtest_kfold = sc.transform(Xtest_kfold)\n",
    "        \n",
    "\n",
    "        #modelling\n",
    "        #model_knn_new = tree.DecisionTreeClassifier(max_depth = 6, criterion='entropy',min_samples_leaf=2, min_samples_split=2 )\n",
    "        model_nb_new = LogisticRegression(C= 10, penalty='l2',solver= 'newton-cg')\n",
    "        model_nb_new.fit(Xtrain_kfold, Ytrain_kfold)\n",
    "\n",
    "        y_pred_nb_new = model_nb_new.predict(Xtest_kfold)\n",
    "\n",
    "        conf_matrix_nb_kfold = confusion_matrix(Ytest_kfold, y_pred_nb_new)\n",
    "\n",
    "        accuracy_nb_kfold = ((conf_matrix_nb_kfold[0,0] + conf_matrix_nb_kfold[1,1])/(conf_matrix_nb_kfold[0,0] +conf_matrix_nb_kfold[0,1]+conf_matrix_nb_kfold[1,0]+conf_matrix_nb_kfold[1,1]))*100\n",
    "\n",
    "        #print(\"Confusion Matrix:\\n \", conf_matrix_knn_kfold)\n",
    "        #print(\"Accuracy \", accuracy_knn_kfold)\n",
    "\n",
    "        row.append(accuracy_nb_kfold)\n",
    "        total += accuracy_nb_kfold\n",
    "    average = row.append(total/parts)\n",
    "    #print(row)\n",
    "    k_list.append(row)\n",
    "    \n",
    "k_list = pd.DataFrame(k_list, columns=['Loops','fold 1','fold 2','fold 3','fold 4', 'mean accuracy'])\n",
    "print(\"SVM Kfold Evaluation for MDVR-KCL Dataset\")\n",
    "k_list\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#print(df.sample(n=7))\n",
    "#print(df)\n",
    "\n",
    "#x = df.take(np.random.permutation(len(df))[:4])\n",
    "#x= df.sample(n=7)\n",
    "#print(x)\n",
    "#print(df.drop(x))\n",
    "\n",
    "#drop_indices = np.random.choice(df.index, 4, replace=False)\n",
    "#df_subset = df.drop(drop_indices)\n",
    "#print(drop_indices)\n",
    "#print(df_subset)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
